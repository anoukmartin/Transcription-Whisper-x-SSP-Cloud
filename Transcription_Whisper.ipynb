{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d9067d-9433-4fb5-95ab-2b7fb06ab40c",
   "metadata": {},
   "source": [
    "# Transcription d'un audio avec Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd3cdd-d023-4b41-a870-e94d00760d94",
   "metadata": {},
   "source": [
    "## 1. Installer Whisper\n",
    "Executer la ligne de code ci-dessous : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7887ac61-0b87-42be-a00e-830241765738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20230314.tar.gz (792 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.9/792.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting triton==2.0.0 (from openai-whisper)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numba in /opt/mamba/lib/python3.11/site-packages (from openai-whisper) (0.57.1)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.11/site-packages (from openai-whisper) (1.24.4)\n",
      "Collecting torch (from openai-whisper)\n",
      "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from openai-whisper) (4.66.1)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Obtaining dependency information for more-itertools from https://files.pythonhosted.org/packages/5a/cb/6dce742ea14e47d6f565589e859ad225f2a5de576d7696e0623b784e226b/more_itertools-10.1.0-py3-none-any.whl.metadata\n",
      "  Downloading more_itertools-10.1.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting tiktoken==0.3.1 (from openai-whisper)\n",
      "  Downloading tiktoken-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from openai-whisper)\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting future (from ffmpeg-python==0.2.0->openai-whisper)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/mamba/lib/python3.11/site-packages (from tiktoken==0.3.1->openai-whisper) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/mamba/lib/python3.11/site-packages (from tiktoken==0.3.1->openai-whisper) (2.31.0)\n",
      "Collecting cmake (from triton==2.0.0->openai-whisper)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/94/87/68536d2dde5acec492742c63bb71f43534eb7d3d83122cce3067c4abca2b/cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from triton==2.0.0->openai-whisper)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting lit (from triton==2.0.0->openai-whisper)\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/mamba/lib/python3.11/site-packages (from numba->openai-whisper) (0.40.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/mamba/lib/python3.11/site-packages (from torch->openai-whisper) (4.7.1)\n",
      "Collecting sympy (from torch->openai-whisper)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch->openai-whisper) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch->openai-whisper)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch->openai-whisper)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch->openai-whisper)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/mamba/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper) (68.1.2)\n",
      "Requirement already satisfied: wheel in /opt/mamba/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->openai-whisper)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.1.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: openai-whisper, future, lit\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=796907 sha256=525f50e13f0c327556ae8aced8bd17d52f4bdc00b07af8ee47e60c5968199dfb\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/37/e4/2b/70a30a9b89099ae923b526d2a8fb63e08ef3412768b6a57fd4\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=18a38742aef48e78ea3e7e51b266ff3aa509893ea42eac6049c2f581ab930952\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/da/19/ca/9d8c44cd311a955509d7e13da3f0bea42400c469ef825b580b\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=ae9e627d245e1cd6a5512e9209330ced3751dcdc7f8013fc375c38e54e38e217\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/ab/84/e4/5af8c76af9e5bee472e825f1451c18bb3b261d80a7b3ec7f8a\n",
      "Successfully built openai-whisper future lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, more-itertools, future, filelock, tiktoken, nvidia-cusolver-cu11, nvidia-cudnn-cu11, ffmpeg-python, triton, torch, openai-whisper\n",
      "Successfully installed cmake-3.27.4.1 ffmpeg-python-0.2.0 filelock-3.12.4 future-0.18.3 lit-16.0.6 more-itertools-10.1.0 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 openai-whisper-20230314 sympy-1.12 tiktoken-0.3.1 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b3dd6-c46e-402b-b986-cc64c62d7540",
   "metadata": {},
   "source": [
    "## 2. Charger le(s) fichier(s) audio à retranscrire\n",
    "Charger le ou les fichier(s) audio à retranscrire. Le(s) fichier(s) doi(ven)t être au format .mp3 ou .wav. Il(s) devrai(en)t apparaître dans la liste de la partie gauche de l'écran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f6cf3-27a3-436f-8da9-f1b2f9b56e41",
   "metadata": {},
   "source": [
    "## 3. Demander à Whisper de retranscrire un audio\n",
    "Executer la ligne de code ci-dessous :\n",
    "- `\"mon_enregistrement_audio.mp3\"` indique le nom du fichier audio. A adapter en fonction du nom de votre fichier.\n",
    "- `--model large-v2` signifie que l'on souhaite utiliser le modèle le plus lourd et précis.\n",
    "- `--language French` signifie que l'entretien est en français.\n",
    "> Pour connaître les autres options disponibles, consulter le dépot Github d'OpenAI : https://github.com/openai/whisper/tree/main\n",
    "\n",
    "\n",
    "Cliquer sur le petit bouton \"play\". Le chargement du modèle va alors commencer, cela peut prendre un certain temps. Une fois terminé la transcription va démmarer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c7d4b-d3e3-4399-a330-cc552629cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!whisper \"mon_enregistrement_audio.mp3\" --model large-v2 --language French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20120bc7-31b9-41ab-93bc-34692344cc55",
   "metadata": {},
   "source": [
    "## 4. Télécharger la transcription de votre fichier audio \n",
    "Lorsque celle-ci est finie, plusieurs fichiers apparaissent dans la partie gauche de l'écran. Ils portent tous le nom de votre audio et sont de formats différents. Le fichier de format texte (.txt) contient la transcription de l'enregistrement. Pour le télécharger : clic droit sur celui-ci > download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264882-be5e-42cb-9dc0-fd487ec38a3b",
   "metadata": {},
   "source": [
    "## 5. Recommencer le point 3. et 4. si vous avez plusieurs fichiers audio à retranscrire"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
